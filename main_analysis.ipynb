{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWLsiaeaVvSfQ2n/ZSU5H/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumpingsphinx/loan_data/blob/main/main_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is the main file of the code I used on this analysis. However, there are a few files used as support for visualization generation, statistical testing, as well as a few key graphical visualizations that can be found on my github repo at [this link.](https://github.com/jumpingsphinx/loan_data)\n",
        "\n",
        "Additionally, you might need to pip install some of the libraries, and mount your drive for this to work. I ran this notebook locally in VSCode and then uploaded it to Colab, so there might be a few tweaks that need to be made for it to run."
      ],
      "metadata": {
        "id": "Jy6Id6gF4Nbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvagB4lO4JiN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, classification_report,\n",
        "                           confusion_matrix, roc_auc_score, roc_curve)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_examine_data(filepath):\n",
        "    \"\"\"\n",
        "    Load the data and perform initial examination\n",
        "    \"\"\"\n",
        "    # Read the data\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Basic information about the dataset\n",
        "    print(\"\\nDataset Overview:\")\n",
        "    print(f\"Number of loans: {len(df)}\")\n",
        "    print(f\"Number of features: {len(df.columns)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_monetary_columns(df):\n",
        "    \"\"\"\n",
        "    Clean monetary columns by removing '$', ',' with regex and converting to float for analysis\n",
        "    \"\"\"\n",
        "    money_columns = ['DISBURSEMENT_AMOUNT', 'BALANCE_AMOUNT', 'CHARGE_OFF_AMOUNT',\n",
        "                    'LOAN_AMOUNT', 'SBA_APPROVED_AMOUNT']\n",
        "\n",
        "    for col in money_columns:\n",
        "        df[col] = df[col].str.replace('[\\$,\\s]', '', regex=True).astype(float)\n",
        "\n",
        "    print(\"\\nMonetary Columns Statistics:\")\n",
        "    print(df[money_columns].describe())\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_dates(df):\n",
        "    \"\"\"\n",
        "    Convert date columns to datetime and create derived features\n",
        "    \"\"\"\n",
        "    date_columns = ['APPROVAL_DATE', 'DEFAULT_DATE', 'DISBURSEMENT_DATE']\n",
        "\n",
        "    for col in date_columns:\n",
        "        df[col] = pd.to_datetime(df[col], format='%d-%b-%y', errors='coerce')\n",
        "\n",
        "    # Create time-based features\n",
        "    df['DAYS_TO_DISBURSEMENT'] = (df['DISBURSEMENT_DATE'] - df['APPROVAL_DATE']).dt.days\n",
        "\n",
        "    # Certain loans had approval years different to their approval date; we'll assume for this analysis that the approval date was correct and overwrite that.\n",
        "    df['APPROVAL_YEAR'] = df['APPROVAL_DATE'].dt.year\n",
        "    df['APPROVAL_MONTH'] = df['APPROVAL_DATE'].dt.month\n",
        "\n",
        "    # Calculate days to default for defaulted loans\n",
        "    df['DAYS_TO_DEFAULT'] = (df['DEFAULT_DATE'] - df['DISBURSEMENT_DATE']).dt.days\n",
        "\n",
        "    # Create default flag\n",
        "    df['DEFAULT_FLAG'] = df['DEFAULT_DATE'].notna().astype(int)\n",
        "\n",
        "    print(\"\\nTime-based Features Statistics:\")\n",
        "    print(df[['DAYS_TO_DISBURSEMENT', 'DAYS_TO_DEFAULT']].describe())\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_categorical_features(df):\n",
        "    \"\"\"\n",
        "    Analyze interesting categorical features and their relationships with defaults\n",
        "    \"\"\"\n",
        "    categorical_cols = ['IS_NEW', 'IS_URBAN',\n",
        "                       'IS_REVOLVER', 'IS_LOW_DOC']\n",
        "\n",
        "    print(\"\\nCategorical Features Analysis:\")\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\n{col} Distribution:\")\n",
        "        print(df[col].value_counts(normalize=True).head())\n",
        "\n",
        "        # Calculate default rates by category\n",
        "        default_rates = df.groupby(col)['DEFAULT_FLAG'].mean()\n",
        "        print(f\"\\nDefault Rates by {col}:\")\n",
        "        print(default_rates)\n",
        "\n",
        "        # Print number of unique values\n",
        "        print(f\"\\nNumber of unique {col} values: {df[col].nunique()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_industry_patterns(df):\n",
        "    \"\"\"\n",
        "    Analyze patterns in industry codes and their relationship with defaults\n",
        "    \"\"\"\n",
        "    # Extract industry sectors (first 2 digits)\n",
        "    df['INDUSTRY_SECTOR'] = df['INDUSTRY_ID'].astype(str).str[:2]\n",
        "\n",
        "    # Calculate metrics by industry sector\n",
        "    industry_metrics = df.groupby('INDUSTRY_SECTOR').agg({\n",
        "        'LOAN_AMOUNT': 'mean',\n",
        "        'DEFAULT_FLAG': 'mean',\n",
        "        'CHARGE_OFF_AMOUNT': 'mean',\n",
        "        'EMPLOYEE_COUNT': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    print(\"\\nIndustry Sector Analysis:\")\n",
        "    print(\"\\nIndustries most likely to default:\")\n",
        "    print(industry_metrics.sort_values('DEFAULT_FLAG', ascending=False).head(10))\n",
        "    print(\"\\nIndustries least likely to default:\")\n",
        "    print(industry_metrics.sort_values('DEFAULT_FLAG', ascending=True).head(10))\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_economic_indicators(df):\n",
        "    \"\"\"\n",
        "    Analyze relationships between economic indicators and loan performance\n",
        "    \"\"\"\n",
        "    economic_cols = ['TREASURY_YIELD', 'CPI_INDEX', 'GDP', 'MORTGAGE_30_US_FIXED',\n",
        "                    'UNRATE', 'INDPRO_INDEX', 'UMCSENT_INDEX', 'CSUSHPINSA_INDEX',\n",
        "                    'CP_INDEX', 'FEDFUNDS_RATE']\n",
        "\n",
        "    # Calculate correlations with default\n",
        "    correlations = pd.DataFrame({\n",
        "        'correlation': [df[col].corr(df['DEFAULT_FLAG']) for col in economic_cols]\n",
        "    }, index=economic_cols)\n",
        "\n",
        "    print(\"\\nEconomic Indicators Correlation with Defaults:\")\n",
        "    print(correlations.sort_values('correlation', ascending=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_derived_features(df):\n",
        "    \"\"\"\n",
        "    Create derived features for modeling\n",
        "    \"\"\"\n",
        "    # Loan characteristics\n",
        "    df['LOAN_PER_EMPLOYEE'] = df['LOAN_AMOUNT'] / df['EMPLOYEE_COUNT'].replace(0, 1)\n",
        "    df['SBA_GUARANTEE_RATIO'] = df['SBA_APPROVED_AMOUNT'] / df['LOAN_AMOUNT']\n",
        "\n",
        "    # Business impact\n",
        "    df['TOTAL_JOBS_IMPACT'] = df['JOBS_CREATED_COUNT'] + df['JOBS_RETAINED_COUNT']\n",
        "    df['JOBS_IMPACT_RATIO'] = df['TOTAL_JOBS_IMPACT'] / df['EMPLOYEE_COUNT'].replace(0, 1)\n",
        "\n",
        "    # Risk indicators\n",
        "    df['CHARGE_OFF_RATIO'] = df['CHARGE_OFF_AMOUNT'] / df['LOAN_AMOUNT']\n",
        "\n",
        "    print(\"\\nDerived Features Statistics:\")\n",
        "    derived_features = ['LOAN_PER_EMPLOYEE', 'SBA_GUARANTEE_RATIO', 'TOTAL_JOBS_IMPACT',\n",
        "                       'JOBS_IMPACT_RATIO', 'CHARGE_OFF_RATIO']\n",
        "    print(df[derived_features].describe())\n",
        "\n",
        "    return df\n",
        "\n",
        "def identify_outliers(df):\n",
        "    \"\"\"\n",
        "    Identify outliers in key numeric features using Tukey's fence method\n",
        "    \"\"\"\n",
        "    numeric_cols = ['LOAN_AMOUNT', 'EMPLOYEE_COUNT', 'TERM', 'JOBS_CREATED_COUNT',\n",
        "                   'JOBS_RETAINED_COUNT']\n",
        "\n",
        "    print(\"\\nOutlier Analysis:\")\n",
        "    for col in numeric_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]\n",
        "        print(f\"\\n{col} outliers: {len(outliers)} ({len(outliers)/len(df):.2%})\")\n",
        "        print(f\"Range: {df[col].min():.2f} to {df[col].max():.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_modeling_data(df):\n",
        "    \"\"\"\n",
        "    Prepare final datasets for modeling\n",
        "    \"\"\"\n",
        "    # Select features for modeling\n",
        "    numeric_features = ['TERM', 'EMPLOYEE_COUNT', 'JOBS_CREATED_COUNT',\n",
        "                       'JOBS_RETAINED_COUNT', 'LOAN_AMOUNT', 'SBA_APPROVED_AMOUNT',\n",
        "                       'TREASURY_YIELD', 'UNRATE', 'GDP', 'LOAN_PER_EMPLOYEE',\n",
        "                       'SBA_GUARANTEE_RATIO', 'TOTAL_JOBS_IMPACT']\n",
        "\n",
        "    categorical_features = ['STATE', 'BANK_STATE', 'INDUSTRY_SECTOR', 'IS_NEW',\n",
        "                          'IS_URBAN', 'IS_REVOLVER', 'IS_LOW_DOC']\n",
        "\n",
        "    # Create modeling dataset\n",
        "    modeling_data = df[numeric_features + categorical_features].copy()\n",
        "    modeling_data = pd.get_dummies(modeling_data, columns=categorical_features)\n",
        "\n",
        "    # Create target variables\n",
        "    targets = {\n",
        "        'default': df['DEFAULT_FLAG'],\n",
        "        'charge_off': df['CHARGE_OFF_AMOUNT']\n",
        "    }\n",
        "\n",
        "    print(\"\\nModeling Data Shape:\", modeling_data.shape)\n",
        "    print(\"Number of features:\", len(modeling_data.columns))\n",
        "\n",
        "    return modeling_data, targets\n",
        "\n",
        "def main_eda(filepath):\n",
        "    \"\"\"\n",
        "    Main function to run the complete EDA pipeline\n",
        "    \"\"\"\n",
        "    # Load and examine data\n",
        "    print(\"Loading and examining data...\")\n",
        "    df = load_and_examine_data(filepath)\n",
        "\n",
        "    # Clean and preprocess\n",
        "    print(\"\\nCleaning monetary columns...\")\n",
        "    df = clean_monetary_columns(df)\n",
        "\n",
        "    print(\"\\nProcessing dates...\")\n",
        "    df = process_dates(df)\n",
        "\n",
        "    # Analyze patterns\n",
        "    print(\"\\nAnalyzing categorical features...\")\n",
        "    df = analyze_categorical_features(df)\n",
        "\n",
        "    print(\"\\nAnalyzing industry patterns...\")\n",
        "    df = analyze_industry_patterns(df)\n",
        "\n",
        "    print(\"\\nAnalyzing economic indicators...\")\n",
        "    df = analyze_economic_indicators(df)\n",
        "\n",
        "    # Create features and identify outliers\n",
        "    print(\"\\nCreating derived features...\")\n",
        "    df = create_derived_features(df)\n",
        "\n",
        "    print(\"\\nIdentifying outliers...\")\n",
        "    df = identify_outliers(df)\n",
        "\n",
        "    # Prepare modeling data\n",
        "    print(\"\\nPreparing modeling data...\")\n",
        "    modeling_data, targets = prepare_modeling_data(df)\n",
        "\n",
        "    return df, modeling_data, targets"
      ],
      "metadata": {
        "id": "P2JeEIB44MLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = r\"C:\\Users\\ashwi\\Downloads\\loan_data.csv\"  # Replace with your file path\n",
        "df, modeling_data, targets = main_eda(filepath)"
      ],
      "metadata": {
        "id": "UCdd5ifb4TH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_economic_data(loan_df, economic_indicators_path):\n",
        "    \"\"\"\n",
        "    Merges loan data with economic indicators based on closest dates and handles missing values.\n",
        "\n",
        "    Parameters:\n",
        "    loan_data_path (str): Path to the loan data CSV\n",
        "    economic_indicators_path (str): Path to the economic indicators CSV\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Merged dataset with handled missing values\n",
        "    \"\"\"\n",
        "    # Read the datasets\n",
        "\n",
        "    econ_df = pd.read_csv(economic_indicators_path)\n",
        "\n",
        "    # Convert approval dates to datetime\n",
        "    loan_df['APPROVAL_DATE'] = pd.to_datetime(loan_df['APPROVAL_DATE'], format='%d-%b-%y')\n",
        "\n",
        "    # Convert economic indicators index to datetime\n",
        "    econ_df['DATE'] = pd.to_datetime(econ_df.iloc[:, 0])\n",
        "\n",
        "    # Create a function to find closest date match\n",
        "    def find_closest_date(target_date):\n",
        "        return abs(econ_df['DATE'] - target_date).idxmin()\n",
        "\n",
        "    # Create new columns for matched economic indicators\n",
        "    economic_columns = ['CPI_INDEX', 'GDP', 'MORTGAGE_30_US_FIXED', 'UNRATE',\n",
        "                       'INDPRO_INDEX', 'UMCSENT_INDEX', 'CSUSHPINSA_INDEX',\n",
        "                       'CP_INDEX', 'FEDFUNDS_RATE']\n",
        "\n",
        "    # Initialize new columns with existing values\n",
        "    for col in economic_columns:\n",
        "        if col in loan_df.columns:\n",
        "            loan_df[f'{col}_matched'] = loan_df[col]\n",
        "        else:\n",
        "            loan_df[f'{col}_matched'] = np.nan\n",
        "\n",
        "    # Match economic indicators based on closest date\n",
        "    for idx, row in loan_df.iterrows():\n",
        "        closest_idx = find_closest_date(row['APPROVAL_DATE'])\n",
        "        for col in economic_columns:\n",
        "            # Only fill if the value is missing or NaN\n",
        "            if pd.isna(loan_df.at[idx, f'{col}_matched']):\n",
        "                loan_df.at[idx, f'{col}_matched'] = econ_df.at[closest_idx, col]\n",
        "\n",
        "    # Replace original columns with matched values where missing\n",
        "    for col in economic_columns:\n",
        "        if col in loan_df.columns:\n",
        "            loan_df[col] = loan_df[col].fillna(loan_df[f'{col}_matched'])\n",
        "        else:\n",
        "            loan_df[col] = loan_df[f'{col}_matched']\n",
        "\n",
        "        # Drop the temporary matched columns\n",
        "        loan_df = loan_df.drop(f'{col}_matched', axis=1)\n",
        "\n",
        "    return loan_df\n",
        "\n",
        "df = merge_economic_data(df, r\"C:\\dev\\python\\loan_data\\fred_data\\economic_indicators_1960_2015.csv\") # Replace with your file path"
      ],
      "metadata": {
        "id": "F-IRF5in4UnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_regression_features(df):\n",
        "    \"\"\"\n",
        "    Prepare features for modeling with detailed NaN checking to ensure we lose as few rows as possible\n",
        "    \"\"\"\n",
        "    economic_indicators = [\n",
        "        'TREASURY_YIELD', 'UNRATE', 'GDP', 'MORTGAGE_30_US_FIXED',\n",
        "        'INDPRO_INDEX', 'UMCSENT_INDEX', 'CSUSHPINSA_INDEX',\n",
        "        'CP_INDEX', 'FEDFUNDS_RATE'\n",
        "    ]\n",
        "\n",
        "    # Create a copy of the dataframe\n",
        "    df_processed = df.copy()\n",
        "    df['Processed'] = (df_processed['LOAN_STATUS'] == 'CHGOFF').astype(int)\n",
        "\n",
        "    # Fill in the economic indicators based on APPROVAL_YEAR\n",
        "    yearly_indicators = df.groupby('APPROVAL_YEAR')[economic_indicators].mean()\n",
        "    for indicator in economic_indicators:\n",
        "        df_processed[indicator] = df_processed['APPROVAL_YEAR'].map(yearly_indicators[indicator])\n",
        "\n",
        "    # Drop rows with NaN in economic indicators (FRED data is not available for all years and therefore some rows may have NaN)\n",
        "    print(f\"\\nNumber of rows before dropping NaN: {len(df_processed)}\")\n",
        "    df_processed = df_processed.dropna(subset=economic_indicators)\n",
        "    print(f\"Number of rows after dropping NaN: {len(df_processed)}\")\n",
        "\n",
        "    # Select features\n",
        "    numeric_features = [\n",
        "        'TERM', 'EMPLOYEE_COUNT', 'JOBS_CREATED_COUNT', 'JOBS_RETAINED_COUNT',\n",
        "        'LOAN_AMOUNT', 'SBA_APPROVED_AMOUNT', 'LOAN_AMOUNT', 'SBA_APPROVED_AMOUNT'\n",
        "    ] + economic_indicators\n",
        "\n",
        "    categorical_features = [\n",
        "        'STATE', 'BANK_STATE', 'INDUSTRY_SECTOR', 'IS_NEW',\n",
        "        'IS_URBAN', 'IS_REVOLVER', 'IS_LOW_DOC'\n",
        "    ]\n",
        "\n",
        "    # Check categorical features for NaN\n",
        "    print(\"\\nChecking categorical features for NaN:\")\n",
        "    print(df_processed[categorical_features].isna().sum())\n",
        "\n",
        "    # Handle numeric features\n",
        "    df_numeric = df_processed[numeric_features].copy()\n",
        "\n",
        "    # Handle categorical features\n",
        "    # Fill NaN in categorical features before encoding\n",
        "    for cat_feature in categorical_features:\n",
        "        df_processed[cat_feature] = df_processed[cat_feature].fillna('Missing')\n",
        "\n",
        "    X_categorical = pd.get_dummies(df_processed[categorical_features], drop_first=True)\n",
        "\n",
        "    # Combine features\n",
        "    X = pd.concat([df_numeric, X_categorical], axis=1)\n",
        "\n",
        "    # Check for NaN in final feature matrix\n",
        "    print(\"\\nChecking combined feature matrix for NaN:\")\n",
        "    nan_cols = X.columns[X.isna().any()].tolist()\n",
        "    if nan_cols:\n",
        "        print(\"Columns with NaN values:\", nan_cols)\n",
        "        print(\"NaN counts in these columns:\")\n",
        "        print(X[nan_cols].isna().sum())\n",
        "\n",
        "        # Drop any remaining rows with NaN\n",
        "        X = X.dropna()\n",
        "        print(f\"\\nShape after dropping all NaN: {X.shape}\")\n",
        "    else:\n",
        "        print(\"No NaN values found in combined feature matrix\")\n",
        "\n",
        "    # Target variable\n",
        "    y = df_processed['CHARGE_OFF_AMOUNT'].fillna(0)\n",
        "    scaler_y = StandardScaler()\n",
        "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "    # Make sure y aligns with X after any row dropping\n",
        "    y = y[X.index]\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_scaled, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(\"\\nFeature preparation completed:\")\n",
        "    print(f\"Training set shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Test set shape: {X_test_scaled.shape}\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, X.columns, scaler, scaler_y\n",
        "\n",
        "def train_random_forest(X_train_scaled, X_test_scaled, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train Random Forest regression model and evaluate its performance.\n",
        "    Initially, evaluated performance of Gradient Boosting, L1 and L2 regularized linear models, but Random Forest performed the best on testing and trained data.\n",
        "    \"\"\"\n",
        "    # Initialize and train model\n",
        "    model = RandomForestRegressor(n_estimators=300, random_state=7, verbose=1, n_jobs=12)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'R2': r2_score(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'MAE': mean_absolute_error(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nRandom Forest Results:\")\n",
        "    print(f\"R² Score: {results['R2']:.4f}\")\n",
        "    print(f\"RMSE: {results['RMSE']:.4f}\")\n",
        "    print(f\"MAE: {results['MAE']:.4f}\")\n",
        "\n",
        "    return results, model\n",
        "\n",
        "def analyze_feature_importance(model, feature_names):\n",
        "    \"\"\"\n",
        "    Analyze feature importance for Random Forest model\n",
        "    \"\"\"\n",
        "    importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(importance.head(10))\n",
        "\n",
        "    return importance\n",
        "\n",
        "def plot_regression_results(model, X_test_scaled, y_test, feature_importance):\n",
        "    \"\"\"\n",
        "    Create visualizations of model performance\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Create figure with multiple subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # 1. Actual vs Predicted\n",
        "    axes[0, 0].scatter(y_test, y_pred, alpha=0.5)\n",
        "    axes[0, 0].plot([y_test.min(), y_test.max()],\n",
        "                   [y_test.min(), y_test.max()],\n",
        "                   'r--', lw=2)\n",
        "    axes[0, 0].set_title('Actual vs Predicted Values')\n",
        "    axes[0, 0].set_xlabel('Actual Charge-off Amount')\n",
        "    axes[0, 0].set_ylabel('Predicted Charge-off Amount')\n",
        "\n",
        "    # 2. Residuals Plot\n",
        "    residuals = y_test - y_pred\n",
        "    axes[0, 1].scatter(y_pred, residuals, alpha=0.5)\n",
        "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0, 1].set_title('Residuals Plot')\n",
        "    axes[0, 1].set_xlabel('Predicted Values')\n",
        "    axes[0, 1].set_ylabel('Residuals')\n",
        "\n",
        "    # 3. Feature Importance Plot\n",
        "    importance = feature_importance.head(10)\n",
        "    sns.barplot(x='importance', y='feature', data=importance, ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Top 10 Feature Importance')\n",
        "\n",
        "    # 4. Prediction Error Distribution\n",
        "    sns.histplot(residuals, kde=True, ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Distribution of Prediction Errors')\n",
        "    axes[1, 1].set_xlabel('Prediction Error')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def cross_validate_model(model, X_train_scaled, y_train, cv=5):\n",
        "    \"\"\"\n",
        "    Perform cross-validation\n",
        "    \"\"\"\n",
        "    cv_scores = cross_val_score(\n",
        "        model, X_train_scaled, y_train,\n",
        "        cv=cv, scoring='r2',\n",
        "        n_jobs=12, verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\nCross-validation Results:\")\n",
        "    print(f\"Average R2: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "    return cv_scores\n",
        "\n",
        "def main_regression(df):\n",
        "    \"\"\"\n",
        "    Main function to run the Random Forest regression analysis\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    print(\"Preparing features...\")\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test, feature_names, scaler, scaler_y = prepare_regression_features(df)\n",
        "\n",
        "    # Train Random Forest model\n",
        "    print(\"\\nTraining Random Forest model...\")\n",
        "    results, model = train_random_forest(X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "    # Analyze feature importance\n",
        "    print(\"\\nAnalyzing feature importance...\")\n",
        "    importance = analyze_feature_importance(model, feature_names)\n",
        "\n",
        "    # Cross-validate model\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    cv_scores = cross_validate_model(model, X_train_scaled, y_train)\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nCreating visualizations...\")\n",
        "    fig = plot_regression_results(model, X_test_scaled, y_test, importance)\n",
        "\n",
        "    print(\"\\nSaving model to disk...\")\n",
        "    with open('random_forest_regression_model.pkl', 'wb') as file:\n",
        "        pickle.dump({\n",
        "            'model': model,\n",
        "            'feature_names': feature_names,\n",
        "            'scaler': scaler,\n",
        "            'scaler_y': scaler_y\n",
        "        }, file)\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "    return results, model, importance, fig, (X_train_scaled, X_test_scaled, y_train, y_test)"
      ],
      "metadata": {
        "id": "QYtmu-hF4V_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df, best_model, importance, fig, (X_train_scaled, X_test_scaled, y_train, y_test) = main_regression(df)"
      ],
      "metadata": {
        "id": "fcY2sKIV4Xcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_classification_data(df):\n",
        "    \"\"\"\n",
        "    Prepare already cleaned data for classification.\n",
        "    Drops high cardinality columns and encodes remaining categorical variables.\n",
        "    \"\"\"\n",
        "    # Drop high cardinality columns and columns that would be unnecessary or unhelpful for classification, or are otherwise redundant\n",
        "    columns_to_drop = ['BORROWER_NAME', 'CITY', 'STATE', 'BANK', 'BANK_STATE',\n",
        "                      'LOAN_STATUS', 'DEFAULT_DATE', 'CHARGE_OFF_AMOUNT',\n",
        "                      'DISBURSEMENT_DATE', 'BALANCE_AMOUNT', 'BALANCE_AMOUNT',\n",
        "                      'APPROVAL_DATE', 'Processed', 'DEFAULT_FLAG', 'DAYS_TO_DEFAULT',\n",
        "                      'CHARGE_OFF_RATIO']\n",
        "\n",
        "    df_cleaned = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Identify remaining categorical columns\n",
        "    categorical_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
        "    print(\"\\nCategorical columns to encode:\", categorical_columns.tolist())\n",
        "\n",
        "    # Create dummy variables for remaining categorical features\n",
        "    X = pd.get_dummies(df_cleaned, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Create target variable (1 for CHGOFF, 0 for PIF)\n",
        "    y = (df['LOAN_STATUS'] == 'CHGOFF').astype(int)\n",
        "\n",
        "    print(\"\\nData Preparation Summary:\")\n",
        "    print(df['LOAN_STATUS'].value_counts())\n",
        "    print(f\"Number of features: {X.shape[1]}\")\n",
        "    print(f\"Number of samples: {X.shape[0]}\")\n",
        "    print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")\n",
        "\n",
        "    return X, y, X.columns.tolist()\n",
        "\n",
        "def train_random_forest_classifier(X_train_scaled, X_test_scaled, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train Random Forest model and evaluate its performance\n",
        "    \"\"\"\n",
        "    # Initialize and train model\n",
        "    model = RandomForestClassifier(n_estimators=300, random_state=7, n_jobs=12, verbose=1)\n",
        "    print(\"\\nTraining Random Forest...\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'accuracy': model.score(X_test_scaled, y_test),\n",
        "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
        "        'classification_report': classification_report(y_test, y_pred),\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    print(\"\\nRandom Forest Results:\")\n",
        "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "    print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(results['classification_report'])\n",
        "\n",
        "    return results, model\n",
        "\n",
        "def analyze_feature_importance(model, feature_names, X, y):\n",
        "    \"\"\"\n",
        "    Analyze feature importance for Random Forest model\n",
        "    \"\"\"\n",
        "    importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(importance.head(10))\n",
        "\n",
        "    X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    correlations = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'correlation': [X_df[col].corr(y) for col in feature_names]\n",
        "    }).sort_values('correlation', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 Positive Correlations with Default:\")\n",
        "    print(correlations.head(10))\n",
        "\n",
        "    print(\"\\nTop 10 Negative Correlations with Default:\")\n",
        "    print(correlations.tail(10))\n",
        "\n",
        "    return importance, correlations\n",
        "\n",
        "def plot_results(model, results, X_test_scaled, y_test, feature_names, X, y):\n",
        "    \"\"\"\n",
        "    Create visualizations of model performance including correlations\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # 1. Feature Importance from Random Forest\n",
        "    importance, correlations = analyze_feature_importance(model, feature_names, X, y)\n",
        "    top_features = importance.head(10)\n",
        "    sns.barplot(x='importance', y='feature', data=top_features, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title('Top 10 Feature Importance (Random Forest)')\n",
        "\n",
        "    # 2. Feature Correlations\n",
        "    correlations_plot = pd.concat([\n",
        "    correlations.nlargest(5, 'correlation'),\n",
        "    correlations.nsmallest(5, 'correlation')\n",
        "])\n",
        "    sns.barplot(x='correlation', y='feature', data=correlations_plot, ax=axes[0, 1],\n",
        "               palette=['red' if x < 0 else 'blue' for x in correlations_plot['correlation']])\n",
        "    axes[0, 1].set_title('Top 5 Positive and Negative Correlations')\n",
        "\n",
        "    # 3. Confusion Matrix\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Confusion Matrix')\n",
        "    axes[1, 0].set_xlabel('Predicted')\n",
        "    axes[1, 0].set_ylabel('Actual')\n",
        "\n",
        "    # 4. ROC Curve\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    axes[1, 1].plot(fpr, tpr, label=f'Random Forest (AUC = {results[\"roc_auc\"]:.3f})')\n",
        "    axes[1, 1].plot([0, 1], [0, 1], 'r--')\n",
        "    axes[1, 1].set_xlabel('False Positive Rate')\n",
        "    axes[1, 1].set_ylabel('True Positive Rate')\n",
        "    axes[1, 1].set_title('ROC Curve')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig, importance, correlations\n",
        "\n",
        "def cross_validate_model(model, X_train_scaled, y_train, cv=5):\n",
        "    \"\"\"\n",
        "    Perform cross-validation\n",
        "    \"\"\"\n",
        "    cv_scores = cross_val_score(\n",
        "        model, X_train_scaled, y_train,\n",
        "        cv=cv, scoring='roc_auc',\n",
        "        n_jobs=12, verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\nCross-validation Results:\")\n",
        "    print(f\"Average ROC AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "    return cv_scores\n",
        "\n",
        "def save_classification_model(model, feature_names, scaler=None, filename='random_forest_classifier_model.pkl'):\n",
        "    \"\"\"\n",
        "    Save the trained classification model and associated data to disk\n",
        "\n",
        "    Parameters:\n",
        "    model: The trained Random Forest classifier\n",
        "    feature_names: List of feature names used in training\n",
        "    scaler: The StandardScaler used to scale features (if any)\n",
        "    filename: Name of the file to save the model to\n",
        "    \"\"\"\n",
        "    print(f\"\\nSaving model to {filename}...\")\n",
        "    try:\n",
        "        save_dict = {\n",
        "            'model': model,\n",
        "            'feature_names': feature_names,\n",
        "        }\n",
        "        if scaler is not None:\n",
        "            save_dict['scaler'] = scaler\n",
        "\n",
        "        with open(filename, 'wb') as file:\n",
        "            pickle.dump(save_dict, file)\n",
        "        print(\"Model saved successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {str(e)}\")\n",
        "\n",
        "def run_classification(X, y):\n",
        "    \"\"\"\n",
        "    Main function to run Random Forest classification analysis on preprocessed data\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=7, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"\\nClass distribution in training set:\")\n",
        "    print(y_train.value_counts(normalize=True))\n",
        "\n",
        "    # Train Random Forest model\n",
        "    print(\"\\nTraining model...\")\n",
        "    results, model = train_random_forest_classifier(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Analyze feature importance\n",
        "    print(\"\\nAnalyzing feature importance...\")\n",
        "    importance = analyze_feature_importance(model, X.columns, X_train, y_train)\n",
        "\n",
        "    # Cross-validate model\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    cv_scores = cross_validate_model(model, X_train, y_train)\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nCreating visualizations...\")\n",
        "    fig, importance, correlations = plot_results(model, results, X_test, y_test, X.columns, X, y)\n",
        "\n",
        "    # Save model - only do if you want to save the model you train locally\n",
        "    # print(\"\\nSaving model...\")\n",
        "    # save_classification_model(model, X.columns)\n",
        "    return results, model, importance, correlations, fig, (X_train, X_test, y_train, y_test)\n"
      ],
      "metadata": {
        "id": "gGrgO4K34YuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for classification and run classification model\n",
        "X, y, feature_names = prepare_classification_data(df)\n",
        "results, best_model, importance, correlations, fig, (X_train, X_test, y_train, y_test) = run_classification(X, y)"
      ],
      "metadata": {
        "id": "28dWIeDH4asS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ak2sVn7D4b0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}